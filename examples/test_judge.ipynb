{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitxt import get_logger\n",
    "from unitxt.api import evaluate, load_dataset\n",
    "from unitxt.blocks import Task, TaskCard\n",
    "from unitxt.inference import (\n",
    "    IbmGenAiInferenceEngineParams,\n",
    "    IbmGenAiInferenceEngine,\n",
    "    HFPipelineBasedInferenceEngine\n",
    ")\n",
    "from unitxt.llm_as_judge import LLMAsJudge\n",
    "from unitxt.loaders import LoadFromDictionary\n",
    "from unitxt.templates import InputOutputTemplate, TemplatesDict\n",
    "from unitxt.text_utils import print_dict\n",
    "import yaml\n",
    "\n",
    "logger = get_logger()\n",
    "from unitxt import add_to_catalog\n",
    "\n",
    "data = {\n",
    "    \"test\": [\n",
    "        {\"conversation\": \"user: is eugene from walking dead really a scientist\"},\n",
    "        {\"conversation\": \"user: is eugene from walking dead really a scientist\"},\n",
    "        {\"conversation\": \"user: is eugene from walking dead really a scientist\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "predictions = [\n",
    "    \"In the Walking Dead series, Eugene Porter initially claims to be a scientist who knows the cure to the zombie plague. However, it is eventually revealed that he is not a scientist, but a high school science teacher who lied about knowing the cure to manipulate other survivors into helping him.\", \n",
    "    \"No, Eugene Porter, played by Josh McDermitt, is not a scientist in real life. In the Walking Dead TV series, he portrays a character who initially appears to be a scientist with knowledge of a cure for the zombie outbreak. However, it is later revealed that he is actually a high school science teacher who lied about his credentials to manipulate other survivors into taking him to Washington D.C.\",\n",
    "    \"thank you for the question.\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_filepath = '../prepare/templates/response_assessment/judges/config_judges.yaml'\n",
    "with open(config_filepath, \"r\") as stream:\n",
    "    configs = yaml.safe_load(stream)\n",
    "# print(configs['metric_version_models'])\n",
    "\n",
    "matrix = configs['metric_version_models']['relevance']\n",
    "# print(matrix)\n",
    "\n",
    "template_lst = [sublist[0] for sublist in matrix]\n",
    "model_lst = [sublist[1] for sublist in matrix]\n",
    "\n",
    "# from unitxt.processors import ToString\n",
    "# add_to_catalog(\n",
    "#     ToString('../prepare/templates/response_assessment/judges/relevance/ensemble_relevancy_v2.pkl'),\n",
    "#     \"templates.response_assessment.judges.relevance.ensemble\",\n",
    "#     overwrite=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitxt.llm_as_judge import MultipleLLMAsJudges\n",
    "\n",
    "platform = 'ibm_gen_ai'\n",
    "gen_params = IbmGenAiInferenceEngineParams(max_new_tokens=256)\n",
    "\n",
    "inference_model_lst = []\n",
    "for model_name in model_lst:\n",
    "    inference_model = IbmGenAiInferenceEngine(model_name=model_name, parameters=gen_params)\n",
    "    inference_model_lst.append(inference_model)\n",
    "\n",
    "# llm_judge_metric = LLMAsJudge(inference_model=inference_model,template=\"templates.response_assessment.judges.relevance.v3\",task=\"rating.single_turn\",main_score=f\"llm_judge_{model_name.split('/')[1].replace('-', '')}{platform}\",strip_system_prompt_and_format_from_inputs=False,)\n",
    "\n",
    "llm_judge_metric = MultipleLLMAsJudges(\n",
    "    inference_model = inference_model_lst,\n",
    "    template = template_lst,\n",
    "    task=\"rating.single_turn\",\n",
    "    # main_score=f\"llm_judge_{model_name.split('/')[1].replace('-', '')}{platform}\",\n",
    "    strip_system_prompt_and_format_from_inputs=False,\n",
    ")\n",
    "\n",
    "\n",
    "card = TaskCard(\n",
    "    loader=LoadFromDictionary(data=data),\n",
    "    task=Task(\n",
    "        inputs={\"conversation\": \"str\"},\n",
    "        outputs={},\n",
    "        prediction_type=\"str\",\n",
    "        metrics=[llm_judge_metric],\n",
    "    ),\n",
    "    templates=TemplatesDict(\n",
    "        {\n",
    "            \"simple\": InputOutputTemplate(\n",
    "                input_format=\"{conversation}\",\n",
    "                output_format=\"\",\n",
    "            )\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_dataset = load_dataset(card=card, template_card_index=\"simple\")[\"test\"]\n",
    "evaluated_dataset = evaluate(predictions=predictions, data=test_dataset)\n",
    "\n",
    "for instance in evaluated_dataset:\n",
    "    print_dict(\n",
    "        instance,\n",
    "        keys_to_print=[\n",
    "            \"source\",\n",
    "            \"prediction\",\n",
    "            \"processed_prediction\",\n",
    "            \"references\",\n",
    "            \"score\",\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
